{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "작성자 : 정에녹\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "앙상블 중 부스팅에 속하는 예측 알고리즘\n",
    "\n",
    "Gradient boosting은 regression과 classification을 지원하는 머신 러닝의 기술명이다. weak prediction model을 ensemble로 구현한 형태의 예측 모델이며, weak prediction model로는 주로 decision tree를 이용한다. 대개 random forest보다 성능이 좋다고 알려져 있다.\n",
    "\n",
    "## scikit-learn에서의 Gradient Boosting 하이퍼 파라미터\n",
    "\n",
    "- `loss: {'squared_error', 'absolute_error', 'huber', 'quantile'}, default='squared_error'`\n",
    "\n",
    "    어떤 loss function을 사용할지 설정한다. `'squared_error'`는 squared error이며, `'absolute_error'`는 absolute error이다. `'huber'`는 이 둘을 같이 사용한다. `'quantile'`은 quantile regression 전용이다(`alpha`를 설정해야 한다).\n",
    "\n",
    "- `learning_rate: float, default=0.1`\n",
    "\n",
    "    각 트리의 learning rate를 설정한다. 값은 (0.0, inf) 범위에 있어야 한다.\n",
    "    \n",
    "- `n_estimators: int, default=100`\n",
    "\n",
    "    부스팅 스테이지의 개수를 설정한다. 값은 [1, inf) 범위에 있어야 한다.\n",
    "\n",
    "- `subsample: float, default=1.0`\n",
    "\n",
    "    각 learner가 학습에 사용할 sample의 양을 설정한다. 값은 (0.0, 1.0] 범위에 있어야 한다.\n",
    "\n",
    "- `criterion: {'friedman_mse', 'squared_error', 'mse'}, default='friedman_mse'`\n",
    "\n",
    "    split의 성능 측정 함수를 설정한다. `'friedman_mse'`는 Friedman score를 통해 개선한 mean squared error이다. `'squared_error'`는 일반적인 mean squared error 이다.\n",
    "\n",
    "- `min_samples_split: int or float, default=2`\n",
    "\n",
    "    internal node를 split하기 위한 최소한의 sample 수를 설정한다.\n",
    "\n",
    "    - int형인 경우, 값은 [2, inf) 범위에 있어야 한다.\n",
    "\n",
    "    - float형인 경우, 값은 (0.0, 1.0] 범위에 있어야 하며, 이 때의 `min_samples_split` 값은 `ceil(min_samples_split * n_samples)`이 된다.\n",
    "\n",
    "- `min_samples_leaf: int or float, default=1`\n",
    "\n",
    "    leaf node가 가질 sample의 최소 개수를 설정한다.\n",
    "    \n",
    "    - int형인 경우, 값은 [1, inf) 범위에 있어야 한다.\n",
    "\n",
    "    - float형인 경우, 값은 (0.0, 1.0] 범위에 있어야 하며, 이 때의 `min_samples_leaf` 값은 `ceil(min_samples_leaf * n_samples)`이 된다.\n",
    "\n",
    "- `min_weight_fraction_leaf: float, default=0.0`\n",
    "\n",
    "    leaf node를 위한 모든 input sample의 weight 합의 최소 weight fraction을 설정한다. 값은 [0.0, 0.5] 범위에 있어야 한다.\n",
    "\n",
    "- `max_depth: int, default=3`\n",
    "\n",
    "    각 estimator의 최대 깊이를 설정한다. 값은 [1, inf) 범위에 있어야 한다.\n",
    "\n",
    "- `min_impurity_decrease: float, default=0.0`\n",
    "\n",
    "    split하기 위한 최소한의 impurity 감소량을 설정한다. 값은 [0.0, inf) 범위에 있어야 한다.\n",
    "\n",
    "- `init: estimator or 'zero', default=None`\n",
    "\n",
    "    초기 예측을 위한 estimator object를 설정한다. `'zero'`라면 초기 예측을 0으로 설정한다. 기본값은 더미 estimator이다.\n",
    "\n",
    "- `random_state: int, RandomState instance or None, default=None`\n",
    "\n",
    "    각 boosting 단계에서 사용할 random seed를 설정한다.\n",
    "\n",
    "- `max_features: {'auto', 'sqrt', 'log2'}, int or float, default=None`\n",
    "\n",
    "    학습에 사용할 feature의 최대 개수 제한을 설정한다.\n",
    "\n",
    "    - int형이라면, 값은 [1, inf) 범위에 있어야 한다.\n",
    "    \n",
    "    - float형이라면, 값은 (0.0, 1.0] 범위에 있어야 한다.\n",
    "    \n",
    "    - `'auto'`라면, `max_features`는 `n_features` 값으로 설정된다.\n",
    "    - `'sqrt'`라면, `max_features`는 `sqrt(n_features)` 값으로 설정된다.\n",
    "    - `'log2'`라면, `max_features`는 `log2(n_features)` 값으로 설정된다.\n",
    "    - `None`이라면, `max_features`는 `n_features` 값으로 설정된다.\n",
    "\n",
    "- `alpha: float, default=0.9`\n",
    "\n",
    "    huber loss function과 quantile loss function에서 사용하는 alpha값을 설정한다. 오직 `loss='huber'`이거나 `loss='quantile'`일 때만 작동한다. 값은 (0.0, 1.0) 범위에 있어야 한다.\n",
    "\n",
    "- `verbose: int, default=0`\n",
    "\n",
    "    결과 출력을 자세히 하도록 설정한다. 값이 1이라면 한 번만 출력하고, 그보다 큰 값이라면 매 트리마다 출력한다. 값은 [0, inf) 범위에 있어야 한다.\n",
    "\n",
    "- `max_leaf_nodes: int, default=None`\n",
    "\n",
    "    leaf node의 최대 개수를 설정한다. 값은 [2, inf) 범위에 있어야 한다. `None`이라면, leaf node의 수는 무제한으로 설정된다.\n",
    "\n",
    "- `warm_start: bool, default=False`\n",
    "\n",
    "    `True`라면, 이전 실행의 결과를 재사용한다.\n",
    "\n",
    "- `validation_fraction: float, default=0.1`\n",
    "\n",
    "    조기 종료를 위한 validation set의 비율을 설정한다. 값은 (0.0, 1.0) 범위에 있어야 한다.\n",
    "\n",
    "- `n_iter_no_change: int, default=None`\n",
    "\n",
    "    조기 종료를 위한 validation score가 개선되지 않은 횟수 조건을 설정한다.\n",
    "\n",
    "    - int형이라면, 값은 [1, inf) 범위에 있어야 한다.\n",
    "\n",
    "    - `None`이라면, 조기 종료 기능을 비활성화한다.\n",
    "\n",
    "- `tol: float, default=1e-4`\n",
    "\n",
    "    조기 종료에서 score 값 비교를 위한 tolerance 값을 설정한다. 값은 (0.0, inf) 범위에 있어야 한다.\n",
    "\n",
    "- `ccp_alpha: non-negative float, default=0.0`\n",
    "\n",
    "    Minimal Cost-Complexity Pruning을 사용할 때의 complexity 파라미터이다. cost complexity 값이 `ccp_alpha`보다 작은 subtree들 중 가장 큰 값을 가진 subtree를 선택한다. 값은 [0.0, inf) 범위에 있어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "XGBoost(Extreme Gradient Boosting)는 기존 Gradient Boosting을 개선하여 병렬 학습이 가능하도록 구현한 프레임워크이다.\n",
    "\n",
    "XGBoost는 다음과 같은 특징을 가진다:\n",
    "\n",
    "- 병렬 처리를 통한 빠른 학습\n",
    "- 과적합 규제\n",
    "\n",
    "## XGBoost의 하이퍼 파라미터\n",
    "\n",
    "- `eta(learning_rate): float, default=0.3`\n",
    "\n",
    "\t각 스텝에 따라 줄어들 학습률을 설정한다. 값은 [0.0, 1.0] 범위에 있어야 한다.\n",
    "\n",
    "- `gamma(min_split_loss): float, default=0.0`\n",
    "\n",
    "\t최소 loss reduction양을 설정한다. 값은 [0.0, inf) 범위에 있어야 한다.\n",
    "\n",
    "- `max_depth: int, default=6`\n",
    "\n",
    "\t트리의 최대 깊이 제한을 설정한다. 값이 `0`인 경우 깊이 제한을 두지 않는다. 값은 [0, inf) 범위에 있어야 한다.\n",
    "\n",
    "- `min_child_weight, default=1`\n",
    "\n",
    "\t자식이 가져야 할 instance weight 합의 최솟값을 설정한다. 값은 [0, inf) 범위에 있어야 한다.\n",
    "\n",
    "- `max_delta_step: int, default=0`\n",
    "\n",
    "\t각 leaf의 output이 될 수 있는 delta step의 최대치를 설정한다. 값이 `0`인 경우 제한을 두지 않는다. 값은 [0, inf) 범위에 있어야 한다.\n",
    "\n",
    "- `subsample: float, default=1.0`\n",
    "\n",
    "\t학습에 사용할 sample의 비율을 설정한다. 값은 (0.0, 1.0] 범위에 있어야 한다.\n",
    "\n",
    "- `colsample_bytree: float, default=1.0`\n",
    "\n",
    "\t각 트리가 사용할 column의 샘플 비율을 설정한다. 값은 (0.0, 1.0] 범위에 있어야 한다.\n",
    "\n",
    "- `colsample_bylevel: float, default=1.0`\n",
    "\n",
    "\t각 split마다 사용할 column의 샘플 비율을 설정한다. `tree_method`가 `'hist'`인 경우 작동하지 않는다. 값은 (0.0, 1.0] 범위에 있어야 한다.\n",
    "\n",
    "- `lambda(reg_lambda): float, default=1.0`\n",
    "\n",
    "\tL2 regularization에 사용할 weight를 설정한다.\n",
    "\n",
    "- `alpha(reg_alpha): float, default=0.0`\n",
    "\n",
    "\tL1 regularization에 사용할 weight를 설정한다.\n",
    "\n",
    "- `tree_method: {'auto', 'exact', 'approx', 'hist', 'gpu_exact', 'gpu_hist'}, default='auto'`\n",
    "\n",
    "\tXGBoost에서 사용할 트리 생성 알고리즘을 설정한다.\n",
    "\n",
    "\t- `'auto'`는 휴리스틱 방법을 사용한다.\n",
    "\n",
    "\t- `'exact'`는 엄격한 그리디 알고리즘을 사용한다.\n",
    "\n",
    "\t- `'approx'`는 quantile sketch와 gradient histogram을 사용하는 대략적인 그리디 알고리즘을 사용한다.\n",
    "\n",
    "\t- `'hist'`는 Fast histogram optimized approximate greedy algorithm을 사용한다.\n",
    "\n",
    "\t- `'gpu_exact'`는 GPU로 구현한 `'exact'` 알고리즘을 사용한다.\n",
    "\n",
    "\t- `'gpu_hist'`는 GPU로 구현한 `'hist'` 알고리즘을 사용한다.\n",
    "\n",
    "- `sketch_eps: float, default=0.03`\n",
    "\n",
    "\t오직 `tree_method`가 `'approx'`일 때만 작동한다. 대략 O(1 / `sketch_eps`)개의 bin을 translate한다. 값은 (0.0, 1.0) 범위에 있어야 한다.\n",
    "\n",
    "- `scale_pos_weight, default=1`\n",
    "\n",
    "\tpositive weight와 negative weight의 균형을 조절하기 위한 상수를 설정한다.\n",
    "\n",
    "- `updater: a comma separated string, default='grow_colmaker,prune'`\n",
    "\n",
    "\ttree updater의 실행 순서를 설정하여 트리를 원하는 형태로 구성하도록 한다.\n",
    "\n",
    "\t- `grow_colmaker`는 non-distributed column-based tree를 생성한다.\n",
    "\n",
    "\t- `distcol`는 column-based data splitting mode를 통해 distributed tree를 생성한다.\n",
    "\n",
    "\t- `grow_histmaker`는 histogram conting의 global proposal에 기반한 row-based data splitting을 통해 distributed tree를 생성한다.\n",
    "\n",
    "\t- `grow_local_histmaker`는 local histogram counting에 기반한 row-based data splitting을 통해 distributed tree를 생성한다.\n",
    "\n",
    "\t- `grow_skmaker`는 approximate sketching algorithm을 사용한다.\n",
    "\n",
    "\t- `sync`는 모든 distributed node의 트리를 동기화한다.\n",
    "\n",
    "\t- `refresh`는 현재 데이터를 통해 트리의 구조를 갱신한다.\n",
    "\n",
    "\t- `prune`는 loss < min_split_loss인 트리를 제거한다.\n",
    "\n",
    "- `refresh_leaf: {0, 1}, default=1`\n",
    "\n",
    "\t`updater`가 `refresh`일 때만 작동한다. `refresh_leaf` 값이 `1`이라면 트리의 leaf와 node가 모두 갱신된다. 값이 `0`이라면 오직 node만 갱신된다.\n",
    "\n",
    "- `process_type: {'default', 'update'}, default='default'`\n",
    "\n",
    "\t실행할 boosting process를 설정한다.\n",
    "\n",
    "\t- `'default'`는 새로운 트리를 생성하는 일반적인 boosting process를 사용한다.\n",
    "\n",
    "\t- `'update'`는 기존 모델에서 시작하여 트리 갱신만 진행한다.\n",
    "\n",
    "- `grow_policy: {'depthwise', 'lossguide'}, default='depthwise'`\n",
    "\n",
    "\t새 node가 트리에 추가되는 방식을 설정한다.\n",
    "\n",
    "\t- `'depthwise'`는 root에 제일 가까운 node를 추가한다.\n",
    "\n",
    "\t- `'lossguide'`는 loss값이 제일 크게 변하는 곳에 node를 추가한다.\n",
    "\n",
    "- `max_leaves: int, default=0`\n",
    "\n",
    "\t`grow_policy`가 `'lossguide'`일 때만 작동한다. 추가할 node의 최대 개수 제한을 설정한다.\n",
    "\n",
    "- `max_bin: int, default=256`\n",
    "\n",
    "\t`tree_method`가 `'hist'`일 때만 작동한다. bucket의 존재할 수 있는 독립적인 bin의 최대 개수 제한을 설정한다.\n",
    "\n",
    "- `predictor: {'cpu_predictor', 'gpu_predictor'}, default='cpu_predictor'`\n",
    "\n",
    "\t어떤 방식의 predictor 알고리즘을 사용할지 설정한다.\n",
    "\n",
    "\t- `'cpu_predictor'`는 멀티코어 CPU 자원을 활용한다.\n",
    "\n",
    "\t- `'gpu_predictor'`는 GPU 자원을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM\n",
    "\n",
    "LightGBM은 tree-based learning algorithm을 사용하는 gradient boosting 프레임워크이다.\n",
    "다른 프레임워크는 decision tree learning을 위해 pre-sort-based algorithms을 사용하는데 반해, LightGBM은 historgram-based 알고리즘을 사용하여 training 속도를 높이고 메모리 사용량을 낮췄다.\n",
    "\n",
    "LightGBM은 다음과 같은 특징을 가진다 :\n",
    "\n",
    "- 빠른 학습 속도\n",
    "- 낮은 메모리 사용량\n",
    "- 높은 정확성\n",
    "- GPU 학습 및 병렬 학습 지원\n",
    "- 빅데이터 처리에 용이\n",
    "\n",
    "## LightGBM의 하이퍼 파라미터\n",
    "\n",
    "- `boosting_type: {'gbdt', 'dart', 'goss', 'rf'}, default='gbdt'`\n",
    "\n",
    "\t어떤 boosting 모델을 사용할지 설정한다.\n",
    "\n",
    "\t- `'gbdt'`는 일반적인 Gradient Boosting Decision Tree를 사용한다.\n",
    "\t\n",
    "\t- `'dart'`는 Dropouts meet Multiple Additive Regression Trees를 사용한다.\n",
    "\t\n",
    "\t- `'goss'`는 Gradient-based One-Side Sampling을 사용한다.\n",
    "\t\n",
    "\t- `'rf'`는 Random Forest를 사용한다.\n",
    "\n",
    "- `num_leaves: int, default=31`\n",
    "\n",
    "\t트리의 최대 leaf 개수를 설정한다.\n",
    "\n",
    "- `max_depth: int, default=-1`\n",
    "\t\n",
    "\t트리의 최대 깊이 제한을 설정한다. 값이 0 이하인 경우는 제한이 없는 것으로 취급한다.\n",
    "\n",
    "- `learning_rate: float, default=0.1`\n",
    "\n",
    "\tboosting에 사용할 학습 계수를 설정한다.\n",
    "\n",
    "- `n_estimators: int, default=100`\n",
    "\n",
    "\tboosting에서 사용할 트리의 수를 설정한다.\n",
    "\n",
    "- `subsample_for_bin: int, default=200000`\n",
    "\n",
    "\tbin을 생성할 때 사용할 sample의 수를 설정한다.\n",
    "\n",
    "- `objective: {'regression', 'binary', 'multiclass', 'lambdarank', callable, None}, default=None`\n",
    "\n",
    "\t학습 목표 함수를 설정한다.\n",
    "\n",
    "\t- `'regression'`은 LGBMRegressor에서 사용한다.\n",
    "\n",
    "\t- `'binary'` 혹은 `'multiclass'`는 LGBMClassifier에서 사용한다.\n",
    "\n",
    "\t- `'lambdarank'`는 LGBMRanker에서 사용한다.\n",
    "\n",
    "- `class_weight: {dict, 'balanced', None}, default=None`\n",
    "\n",
    "\t각 class의 weight를 설정한다.\n",
    "\n",
    "\t- `{class_label:weight, ...}`은 해당 `class_label`의 가중치를 `weight`으로 설정한다.\n",
    "\n",
    "\t- `'balanced'`는 class의 가중치를 비율에 맞춰 자동으로 설정한다.\n",
    "\n",
    "\t- `None`'은 모든 class의 가중치를 1로 설정한다.\n",
    "\n",
    "- `min_split_gain: float, default=0.0`\n",
    "\n",
    "\t트리의 leaf node를 split하기 위한 최소 loss reduction의 값을 설정한다.\n",
    "\n",
    "- `min_child_weight: float, default=1e-3`\n",
    "\n",
    "\t자식이 가져야 할 instance weight들 합의 최소치를 설정한다.\n",
    "\n",
    "- `min_child_samples: int, default=20`\n",
    "\n",
    "\t자식이 가져야 할 최소한의 데이터 요구량을 설정한다.\n",
    "\n",
    "- `subsample: float, default=1.0`\n",
    "\n",
    "\ttraining instance의 subsample 비율을 설정한다.\n",
    "\n",
    "- `subsample_freq: int, default=0`\n",
    "\n",
    "\tsubsample의 주기를 설정한다. 값이 0 이하라면 subsample을 비허용한다.\n",
    "\n",
    "- `colsample_bytree: float, default=1.0`\n",
    "\n",
    "\t각 트리를 생성할 때 column의 subsample 비율을 설정한다.\n",
    "\n",
    "- `reg_alpha: float, default=0.0`\n",
    "\n",
    "\tL1 regularization에서의 weights를 설정한다.\n",
    "\n",
    "- `reg_lambda: float, default=0.0`\n",
    "\n",
    "\tL2 regularization에서의 weights를 설정한다.\n",
    "\n",
    "- `random_state: {int, RandomState object, None}, default=None`\n",
    "\n",
    "\t랜덤 시드 값을 설정한다.\n",
    "\n",
    "\t- `int`형인 경우, 해당 값을 시드로 설정한다.\n",
    "\n",
    "\t- `RandomState object`인 경우, 해당 state에 따른 integer값을 시드로 설정한다.\n",
    "\n",
    "\t- `None`인 경우, 기본 시드값을 사용한다.\n",
    "\n",
    "- `n_jobs: {int, None}, default=None`\n",
    "\n",
    "\ttraining에 사용할 thread 수를 설정한다.\n",
    "\n",
    "\t- 음수인 경우, `n_cpus + 1 + n_jobs` 값으로 설정한다.\n",
    "\n",
    "\t- 0인 경우, OpenMP의 기본 값으로 설정한다.\n",
    "\n",
    "\t- `None`인 경우, physical core의 개수로 설정한다.\n",
    "\n",
    "- `importance_type: {'split', 'gain'}, default='split'`\n",
    "\n",
    "\tfeature importance의 타입을 설정한다.\n",
    "\n",
    "\t- `'split'`인 경우, 결과는 해당 feature가 모델에서 사용된 횟수를 포함한다.\n",
    "\n",
    "\t- `'gain'`인 경우, 결과는 모든 feature가 모델에서 사용된 횟수를 포함한다."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
